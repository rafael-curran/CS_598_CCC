import grpc
import data_feed_pb2
import data_feed_pb2_grpc

class DecisionEngine:
    def __init__(self, sample_metrics, gpu_t, cpu_t, io_t, cpu_cores_compute, cpu_cores_storage, grpc_host, grpc_port):
        """
        :param sample_metrics: List of tuples (original_size, transformed_size, preprocessing_time)
        :param tg: GPU time for one epoch (TG)
        :param tcc: CPU time on compute node (TCC)
        :param tcs: CPU time on storage node (TCS)
        :param tnet: Network transfer time (TNet)
        :param cpu_cores_compute: Number of CPU cores on compute node
        :param cpu_cores_storage: Number of CPU cores on storage node
        :param grpc_host: Hostname for the gRPC server
        :param grpc_port: Port for the gRPC server
        """
        self.sample_metrics = sample_metrics
        self.num_samples = len(sample_metrics)
        self.tg = self.num_samples / gpu_t  # Total GPU time for all samples
        self.tcc = self.num_samples / cpu_t 
        self.tcs = self.num_samples / cpu_t
        self.tnet = self.num_samples / io_t
        self.cpu_cores_compute = cpu_cores_compute
        self.cpu_cores_storage = cpu_cores_storage
        self.grpc_host = grpc_host
        self.grpc_port = grpc_port
        self.offloading_plan = {}
        
    def decide_offloading(self, current_offloading_plan):
        decisions = []
        print(f"Sample_metrics data type = {type(self.sample_metrics)}")

        # Iterate through each sample in sample_metrics
        for sample_id, sample in enumerate(self.sample_metrics):
            original_size = sample['original_size']
            transformed_sizes = sample['transformed_sizes']
            preprocessing_times = sample['preprocessing_times']
            current_transformations = current_offloading_plan.get(sample_id, 0)

            print(f"Processing sample {sample_id}: original_size={original_size}, current_transformations={current_transformations}")

            for i in range(current_transformations, len(preprocessing_times)):
                if i == 0:
                    size_reduction = original_size - transformed_sizes[i]
                else:
                    size_reduction = transformed_sizes[i - 1] - transformed_sizes[i]

                marginal_saved_tnet = (size_reduction / original_size) * self.tnet
                marginal_efficiency = marginal_saved_tnet / preprocessing_times[i]

                # Debugging output for decision process
                print(f"Sample {sample_id}, transformation {i}: size_reduction={size_reduction}, marginal_saved_tnet={marginal_saved_tnet}, "
                    f"marginal_efficiency={marginal_efficiency}, preprocessing_time={preprocessing_times[i]}")

                if marginal_efficiency > 0:
                    decisions.append((sample_id, i+1, marginal_efficiency, marginal_saved_tnet, preprocessing_times[i]))
                    print(f"Sample {sample_id}: Added to offloading plan with transformation {i+1} and marginal_efficiency={marginal_efficiency}")
                else:
                    print(f"Sample {sample_id}: Not selected for offloading because marginal_efficiency={marginal_efficiency}")

        return sorted(decisions, key=lambda x: -x[2])



    def iterative_offloading(self):
        current_tnet = self.tnet
        current_tcc = self.tcc / self.cpu_cores_compute  
        current_tcs = 0  
        current_offloading_plan = {}  

        offloading_plan = {}
        decisions = self.decide_offloading(current_offloading_plan)

        for sample_id, next_transformation, marginal_efficiency, marginal_saved_tnet, marginal_preprocessing_time in decisions:
            current_tnet -= marginal_saved_tnet
            current_tcc -= marginal_preprocessing_time / self.cpu_cores_compute
            current_tcs += marginal_preprocessing_time / self.cpu_cores_storage

            print(f"Sample {sample_id} selected for offloading: next_transformation={next_transformation}, "
                f"marginal_efficiency={marginal_efficiency}, current_tnet={current_tnet}, current_tcs={current_tcs}")

            current_offloading_plan[sample_id] = next_transformation
            offloading_plan[sample_id] = next_transformation

            # Check if offloading should stop based on the TNet and TCS comparison
            if current_tnet < current_tcs:
                print(f"Stopping offloading: current_tnet={current_tnet} is less than current_tcs={current_tcs}")
                break

        return offloading_plan



    def send_offloading_requests(self):
        # Create a gRPC channel to communicate with the storage server
        channel = grpc.insecure_channel(f'{self.grpc_host}:{self.grpc_port}')
        stub = data_feed_pb2_grpc.DataFeedStub(channel)

        # Create a stream for offloading requests
        offloading_stream = stub.get_samples(iter(self._generate_offloading_requests()))

        # Iterate through the stream to make sure the server acknowledges the updates
        for response in offloading_stream:
            print(f"Server Response: {response.status}")

    def _generate_offloading_requests(self):
        # Create offloading plan and generate gRPC messages for the server
        offloading_plan = self.iterative_offloading()
        for sample_id, transformations in offloading_plan.items():
            yield data_feed_pb2.OffloadingRequest(
                sample_id=sample_id,
                transformations=transformations
            )

if __name__ == '__main__':
    # Example input from profiling and broader system metrics
    sample_metrics = [(10000, 2000, 1.5), (9000, 3000, 0.5), (8000, 4000, 0.7)]  # Example data
    tg = 10  # Example GPU time for one epoch
    tcc = 5  # Example CPU time for local preprocessing (compute node)
    tcs = 0  # Initial CPU time on storage node (no offloading)
    tnet = 8  # Example network transfer time (TNet)
    cpu_cores_compute = 8  # Example number of CPU cores on compute node
    cpu_cores_storage = 4  # Example number of CPU cores on storage node

    # Initialize the DecisionEngine with gRPC details
    engine = DecisionEngine(sample_metrics, tg, tcc, tcs, tnet, cpu_cores_compute, cpu_cores_storage, grpc_host='localhost', grpc_port=50051)
    
    # Send offloading requests
    engine.send_offloading_requests()
